{
    "cells": [
        {
            "cell_type": "code",
            "source": [
                "from notebookutils import mssparkutils\n",
                "from pyspark.sql.functions import udf, col, from_json, concat_ws, explode, current_timestamp\n",
                "from pyspark.sql.types import StringType, Row, StructType, StructField, ArrayType, MapType\n",
                "from pyspark.sql.utils import AnalysisException\n",
                "\n",
                "\n",
                "from synapse.ml.services import AnalyzeDocument\n",
                "\n",
                "from delta.tables import *\n",
                "\n",
                "from synapse.ml.services.openai import OpenAIChatCompletion\n",
                "import json\n",
                "import pyspark"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "id": "f886d32f-24b8-431b-b2d9-cdf709bf8768"
        },
        {
            "cell_type": "code",
            "source": [
                "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\",\"true\")"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "id": "95e13306-cbe0-4658-89b3-68cc40f466b8"
        },
        {
            "cell_type": "code",
            "source": [
                "# Getting all necessary secrets \n",
                "\n",
                "ai_services_key = mssparkutils.credentials.getSecret('https://keyvaultnew.vault.azure.net/', 'DocIntelligenceKey')\n",
                "ai_services_location = mssparkutils.credentials.getSecret('https://keyvaultnew.vault.azure.net/', 'DocIntelligenceRegion') \n",
                "ai_aoai_key = mssparkutils.credentials.getSecret('https://keyvaultnew.vault.azure.net/', 'AOAIKey')\n",
                "ai_aoai_url = mssparkutils.credentials.getSecret('https://keyvaultnew.vault.azure.net/', 'AOAIURL')"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "id": "ef6ff2d9-67a9-4e3f-a658-0e4844c06567"
        },
        {
            "cell_type": "code",
            "source": [
                "# Input parameter\n",
                "my_document_path = \"Files/PDF/MYPDFFILE.pdf\""
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                },
                "tags": [
                    "parameters"
                ]
            },
            "id": "70b9ca04-7129-46e1-8a9c-4739513ff9ae"
        },
        {
            "cell_type": "code",
            "source": [
                "# Define the JSON structure you want to extract\n",
                "my_json_structure = {\n",
                "  \"myjsonstructure\": {\n",
                "    \"id\": \"\",\n",
                "    \"date\": \"\",\n",
                "    \"attribute 1\": \"\",\n",
                "    \"attribute 2\": \"\",\n",
                "  }\n",
                "}"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "id": "09006831-9147-46e8-9687-6d216d95a26e"
        },
        {
            "cell_type": "code",
            "source": [
                "my_json_schema = ArrayType(\n",
                "    StructType([\n",
                "        StructField(\"myjsonstructure\",\n",
                "            StructType([\n",
                "                StructField(\"id\", StringType(), True),\n",
                "                StructField(\"attribute 1\", StringType(), True),\n",
                "                StructField(\"attribute 2\", StringType(), True),\n",
                "            ])\n",
                "        )\n",
                "    ])\n",
                ")"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "id": "f2d0079c-d0e3-4a35-98b5-971ce0932cda"
        },
        {
            "cell_type": "code",
            "source": [
                "new_dfs_info = [\n",
                "    {\"newDataFrameName\": \"df_myjsonstructure\", \"columnNames\": [\"parsedContent.root.id\", \"parsedContent.root.attribute 1\", \"parsedContent.root.attribute 2\", \"...\", current_timestamp().alias(\"insert_datetime\")]},\n",
                "]"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "094281ed-d73d-41fb-91d2-e1477cbebcfe"
        },
        {
            "cell_type": "code",
            "source": [
                "def make_message(role, content) -> pyspark.sql.Row:\n",
                "    return Row(role=role, content=content, name=role)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "id": "db513f4b-fb4a-4ad1-b6aa-d2736004f715"
        },
        {
            "cell_type": "code",
            "source": [
                "def get_structured_content(document_path: str, json_structure: dict[str, any], spark_schema: pyspark.sql.types.DataType, extra_prompt_information: str | None, ai_services_location: str, ai_services_key: str, ai_aoai_url: str | None, ai_aoai_key: str | None) -> pyspark.sql.DataFrame:\n",
                "    df = (\n",
                "        spark.read.format(\"binaryFile\")\n",
                "        .load(document_path)\n",
                "        .limit(10)\n",
                "        .cache()\n",
                "    )\n",
                "\n",
                "    analyze_document = (\n",
                "        AnalyzeDocument()\n",
                "        .setPrebuiltModelId(\"prebuilt-layout\")\n",
                "        .setSubscriptionKey(ai_services_key)\n",
                "        .setLocation(ai_services_location)\n",
                "        .setImageBytesCol(\"content\")\n",
                "        .setOutputCol(\"result\")\n",
                "        .setPages(\"1-5\") # for sake of quick processing, only read the first 15 pages of the documents\n",
                "    )\n",
                "\n",
                "    analyzed_df = (\n",
                "        analyze_document.transform(df)\n",
                "        .withColumn(\"output_content\", col(\"result.analyzeResult.content\"))\n",
                "        .withColumn(\"paragraphs\", col(\"result.analyzeResult.paragraphs\"))).cache()\n",
                "\n",
                "    analyzed_df = analyzed_df.drop(\"content\")\n",
                "\n",
                "    messages = []\n",
                "\n",
                "    for i in analyzed_df.collect(): \n",
                "        messages.append(\n",
                "            [\n",
                "                (\n",
                "                    [\n",
                "                        make_message(\n",
                "                            \"system\", f\"You are a useful assistant supporting with structured extraction of information from texts. Don't add any comments or explaining text. Always only return the expected JSON filled with the content that was asked for. {extra_prompt_information or ''}\"\n",
                "                        ),\n",
                "                        make_message(\"user\", f\"Extract the following information in JSON format: {json.dumps(json_structure)} from the following text: {i['output_content']}\"),\n",
                "                    ]\n",
                "                )\n",
                "            ]\n",
                "            )\n",
                "\n",
                "    colname = [\"messages\"]\n",
                "    chat_df = spark.createDataFrame(messages, colname)\n",
                "\n",
                "    open_ai_chat_completion = (\n",
                "        OpenAIChatCompletion()\n",
                "            .setDeploymentName(\"gpt-4-32k\")\n",
                "            .setMessagesCol(\"messages\")\n",
                "            .setErrorCol(\"error\")\n",
                "            .setOutputCol(\"chat_completions\")\n",
                "    )\n",
                "    if ai_aoai_url:\n",
                "        # Using a provisioned AOAI gpt-4-32k model in case Fabric Copilot is not available\n",
                "        open_ai_chat_completion = (\n",
                "            open_ai_chat_completion\n",
                "                .setUrl(ai_aoai_url)\n",
                "                .setSubscriptionKey(ai_aoai_key)\n",
                "        )\n",
                "\n",
                "    intermediate_df = open_ai_chat_completion.transform(chat_df).select(\"messages\", \"chat_completions.choices.message.content\")\n",
                "    intermediate_df = intermediate_df.withColumn(\"content_str\", concat_ws(\"\", col(\"content\")))\n",
                "\n",
                "    new_df = intermediate_df.withColumn(\"parsedContent\", from_json(col(\"content_str\"), spark_schema))\n",
                "\n",
                "    new_df.cache()\n",
                "\n",
                "    return new_df.select(explode(\"parsedContent\").alias(\"parsedContent\"))"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "7cb22879-ebd9-4e7f-a5ae-a397508ac2d4"
        },
        {
            "cell_type": "code",
            "source": [
                "def create_new_dataframes(source_dataframe: pyspark.sql.DataFrame, output_dataframe_config: list[dict[str, any]]) -> list[pyspark.sql.DataFrame]:\n",
                "    # Dictionary to store the new DataFrames\n",
                "    new_dfs = {}\n",
                "\n",
                "    for row in output_dataframe_config:\n",
                "        new_df_name = row[\"newDataFrameName\"]\n",
                "        column_names = row[\"columnNames\"]\n",
                "\n",
                "        # Select the specified columns from the source DataFrame\n",
                "        new_df = source_dataframe.select(*column_names)\n",
                "        \n",
                "        # Store the new DataFrame in the dictionary\n",
                "        new_dfs[new_df_name] = new_df\n",
                "    \n",
                "    return new_dfs"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "606c66dc-8a4f-41b2-840a-923aa0b97711"
        },
        {
            "cell_type": "code",
            "source": [
                "def write_dataframes(dataframes: dict[str, pyspark.sql.DataFrame]):\n",
                "    output_path = 'Tables/'\n",
                "\n",
                "    for df_name, df in dataframes.items():\n",
                "        # Write each DataFrame as a Delta Lake table\n",
                "        df \\\n",
                "            .write \\\n",
                "            .format(\"delta\") \\\n",
                "            .option(\"mergeSchema\", \"true\") \\\n",
                "            .mode(\"append\") \\\n",
                "            .save(f\"{output_path}/{df_name}\")\n"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "f9221a18-ab7f-438a-9db3-e52b1ad19660"
        },
        {
            "cell_type": "code",
            "source": [
                "def get_structured_content_and_write_to_default_lakehouse(document_path: str, json_structure: dict[str, any], spark_schema: pyspark.sql.types.DataType, extra_prompt_information: str | None, output_dataframe_config: list[dict[str, any]], ai_services_location: str, ai_services_key: str, ai_aoai_url: str | None, ai_aoai_key: str | None):\n",
                "    df = get_structured_content(document_path, json_structure, spark_schema, None, ai_services_location, ai_services_key, ai_aoai_url, ai_aoai_key)\n",
                "    new_dfs = create_new_dataframes(df, output_dataframe_config)\n",
                "    write_dataframes(new_dfs)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "4571965d-6f6a-4f69-9d28-56e7925f2c7e"
        },
        {
            "cell_type": "code",
            "source": [
                "get_structured_content_and_write_to_default_lakehouse(my_document_path, my_json_structure, my_json_schema, None, new_dfs_info, ai_services_location, ai_services_key, ai_aoai_url, ai_aoai_key)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "90edae70-b2ca-4795-a861-809e6cc890da"
        }
    ],
    "metadata": {
        "kernel_info": {
            "name": "synapse_pyspark"
        },
        "kernelspec": {
            "name": "synapse_pyspark",
            "language": "Python",
            "display_name": "Synapse PySpark"
        },
        "language_info": {
            "name": "python"
        },
        "microsoft": {
            "language": "python",
            "language_group": "synapse_pyspark",
            "ms_spell_check": {
                "ms_spell_check_language": "en"
            }
        },
        "notebook_environment": {},
        "nteract": {
            "version": "nteract-front-end@1.0.0"
        },
        "widgets": {},
        "synapse_widget": {
            "state": {},
            "version": "0.1"
        },
        "save_output": true,
        "spark_compute": {
            "compute_id": "/trident/default",
            "session_options": {
                "conf": {
                    "spark.synapse.nbs.session.timeout": "1200000"
                },
                "enableDebugMode": false
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}